{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPq+pA8V+C1NOqHc/pqFrJb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuilhermeGAraujo/Trab_NLP/blob/main/BERT_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelo adaptado de https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/"
      ],
      "metadata": {
        "id": "dexb1KiT2ehr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "Du_Vw0Z2RBPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FBuiwQpGQKWN"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "oE5ygJNynvwr"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "db8AmBh4m8xI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D12CkVvomknE",
        "outputId": "fe1348c5-b30c-4b5e-b65c-aafefadfeb9e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/pos/NLP/data.csv')"
      ],
      "metadata": {
        "id": "wAwCFfTDsDQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_dict={'negative':0,'neutral':1,'positive':2}\n",
        "df['Sentiment']=df['Sentiment'].apply(lambda x: sent_dict[x])"
      ],
      "metadata": {
        "id": "ap6aCTVPsKBi"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_test = train_test_split( df, test_size=0.2, random_state=42)\n",
        "df_val, df_test = train_test_split(df_test,test_size=0.5,random_state=42)"
      ],
      "metadata": {
        "id": "SkGJvSikUFQy"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', return_dict=False)"
      ],
      "metadata": {
        "id": "fWBbyQuzQ_ye"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset_Fin(Dataset):\n",
        "\n",
        "  def __init__(self, frases, targets, tokenizer, max_len):\n",
        "              self.frases = frases\n",
        "              self.targets = targets\n",
        "              self.tokenizer = tokenizer\n",
        "              self.max_len = max_len\n",
        "             \n",
        "    \n",
        "  def __len__(self):\n",
        "\n",
        "      return len(self.frases)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    frase = str(self.frases[item])\n",
        "    target = self.targets[item]\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "                frase,\n",
        "                add_special_tokens=True,\n",
        "                max_length=self.max_len,\n",
        "                return_token_type_ids=False,\n",
        "                pad_to_max_length=True,\n",
        "                return_attention_mask=True,\n",
        "                return_tensors='pt',\n",
        "\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'frase_text': frase,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }"
      ],
      "metadata": {
        "id": "e-365DhBQ-fC"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "\n",
        "  ds = Dataset_Fin( frases=df.Sentence.to_numpy(), \n",
        "                       targets=df.Sentiment.to_numpy(), \n",
        "                       tokenizer=tokenizer,\n",
        "                         max_len=max_len )\n",
        "\n",
        "  return DataLoader( ds, batch_size=batch_size,num_workers=4)\n"
      ],
      "metadata": {
        "id": "NCcqEt4TQ7sP"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "MAX_LEN = 86 #dobro do numero de token da frase com amior nÃºmero de tokens (primeiro notebook)\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkQMlrWQobq7",
        "outputId": "614c4e96-9eae-461a-ae64-fb1fe57bdc67"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(SentimentClassifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "\n",
        "  def forward(self, input_ids, attention_mask,return_dict):\n",
        "    _, pooled_output = self.bert( input_ids=input_ids,\n",
        "                                 attention_mask=attention_mask,return_dict=False )\n",
        "    output = self.drop(pooled_output)\n",
        "    return self.out(output)"
      ],
      "metadata": {
        "id": "-JwtYzPPim3h"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentimentClassifier(3)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-P704TwDoDZk",
        "outputId": "af98d946-117f-4f77-aac2-66e47a3c10e7"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler,\n",
        "                n_examples):\n",
        "\n",
        "  model = model.train()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  for d in data_loader:\n",
        "\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "\n",
        "    targets = d[\"targets\"].to(device)\n",
        "\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask,\n",
        "                    return_dict=False)\n",
        "\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "metadata": {
        "id": "qY0BU_WokHSn"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "\n",
        "  model = model.eval()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for d in data_loader:\n",
        "\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model( input_ids=input_ids, attention_mask=attention_mask,\n",
        "                      return_dict=False)\n",
        "\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "metadata": {
        "id": "5zhT9ToWkmA0"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup( optimizer, num_warmup_steps=0,\n",
        "                                             num_training_steps=total_steps)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty-V2a6xjacZ",
        "outputId": "fe98e670-9939-4813-8a87-171cb2bb2841"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "history = defaultdict(list)\n",
        "\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "  train_acc, train_loss = train_epoch( model, train_data_loader, loss_fn,\n",
        "                                      optimizer, device,scheduler,len(df_train))\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model( model, val_data_loader, loss_fn, device,\n",
        "                                 len(df_val))\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "    best_accuracy = val_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3YUyd1kk3Lb",
        "outputId": "2c67f0ff-8f59-478f-ccc0-1c720527f6c1"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.6292488134558291 accuracy 0.7164562379627648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.44316839124705343 accuracy 0.7893835616438356\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.3567850929605473 accuracy 0.838861545046009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.495684883985165 accuracy 0.8013698630136986\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.2527192535590851 accuracy 0.8760967258720308\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.5364162358077796 accuracy 0.8047945205479452\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.19674826819336824 accuracy 0.8962122833297668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.5944194923582915 accuracy 0.8304794520547945\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.16926187637250922 accuracy 0.9099079820243955\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.6215500956954988 accuracy 0.8116438356164383\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.15455622357056467 accuracy 0.9131179114059491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.6945573983940523 accuracy 0.8133561643835616\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.15434264339512188 accuracy 0.9111919537770169\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.6804610701479219 accuracy 0.8082191780821917\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.14492091712177932 accuracy 0.9144018831585705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.7375314902715586 accuracy 0.7996575342465753\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.14161783867672964 accuracy 0.9126899208217419\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.7699696208992218 accuracy 0.7962328767123287\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.1356453400126296 accuracy 0.9118339396533276\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.7818476598146623 accuracy 0.785958904109589\n",
            "\n",
            "CPU times: user 9min 12s, sys: 3min 30s, total: 12min 42s\n",
            "Wall time: 13min 17s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_acc=[x.tolist() for x in history['train_acc']]\n",
        "val_acc=[x.tolist() for x in history['val_acc']]\n",
        "plt.plot(train_acc, label='AcurÃ¡cia treino')\n",
        "plt.plot(val_acc, label='AcurÃ¡cia validaÃ§Ã£o')\n",
        "plt.title('HistÃ³rico do treinamento')\n",
        "plt.ylabel('AcurÃ¡cia')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "tUBd-1A_ibg5",
        "outputId": "b3d0aff1-5933-444e-8975-112657faef92"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc, _ = eval_model(model, test_data_loader, loss_fn, device, len(df_test))\n",
        "\n",
        "test_acc.item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gx5_Piwx8ZJ",
        "outputId": "e08fa75e-75b6-46d4-c986-ae11d6f7e1ef"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7572649572649572"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictions(model, data_loader):\n",
        "\n",
        "  model = model.eval()\n",
        "  review_texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      texts = d[\"frase_text\"]\n",
        "      \n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model( input_ids=input_ids, attention_mask=attention_mask,\n",
        "                      return_dict=True)\n",
        "\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      frase_texts.extend(texts)\n",
        "\n",
        "      predictions.extend(preds)\n",
        "\n",
        "      prediction_probs.extend(outputs)\n",
        "\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "\n",
        "  return frase_texts, predictions, prediction_probs, real_values"
      ],
      "metadata": {
        "id": "LmiHp64dyDbt"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['negativo','neutro','positivo']\n",
        "y_frases_texts, y_pred, y_pred_probs, y_test =get_predictions( \n",
        "                                                model,test_data_loader)\n",
        "print(classification_report(y_test, y_pred, target_names=classes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NLGuV6c0yV2",
        "outputId": "d1b0b003-66b1-4559-f846-b1e35348b5c5"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negativo       0.36      0.41      0.38        79\n",
            "      neutro       0.82      0.80      0.81       312\n",
            "    positivo       0.84      0.84      0.84       194\n",
            "\n",
            "    accuracy                           0.76       585\n",
            "   macro avg       0.67      0.68      0.68       585\n",
            "weighted avg       0.76      0.76      0.76       585\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O modelo Bert teve um desempenho melhor que o Vader, com acurÃ¡cia de 76% e alto recall para sentimentos neutro e positivo. Apesar do recall de negativos estÃ¡ melhor que Vader, ainda Ã© baixo.\n",
        "\n",
        "Acredito qe a dificuldade dos dois modelos  em separar as frases negativas das demais vem do fato de haver grande semelhanÃ§a entre os termos utilizados independentes da classe de sentimento e de a proporÃ§Ã£o de negativos ser menor que a de positivos ou neutros gerando uma bais durante o treinamento para o modelo predizer neutro ou positivo ao invÃ©s de negativo"
      ],
      "metadata": {
        "id": "wCTg2vEe2O2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nGH90VJW6PC3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}